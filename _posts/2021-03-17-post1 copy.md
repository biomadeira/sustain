---
title:  Engenharia de Dados & Docker
subtitle: Criando um cluster de Spark simples no Docker
---

Quando falamos de Big Data e Engenharia de Dados nos dias atuais uma figura marcante e muitas vezes dita como a centralizadora de uma pipeline de dados é o `Apache Spark`. Esse framework que muitas vezes coloca medo nos Engenheiros de Dados tem por característica ser um simplificador para ingestão, transformação e carregamento de dados em processamento de dados em larga escala.


Contudo, por onde começar? Muitas vezes é a pergunta que muitos estudantes fazem. A arquitetura e funcionamento do Spark é um contexto complexo, porém, se aplicado em etapas o aprendizado se torna fácil e exponencial.


**Afinal, o que é o Spark?**



No site do Apache Spark:

> **Apache Spark <sup>TM</sup>** é um mecanismo de análise unificado para processamento de dados distribuídos em grande escala.

Em termos, Spark é uma framework(estrutura) de computação distribuída que possui interface para várias APIs, incluindo linguagem Python, SQL, R, .Net. Uma vez que o dado é distribuído no sistema, Spark pode trabalhar com grandes datasets ao longo de um cluster de máquinas.


**Porque Spark?**


Pela visão da Engenharia, Spark está disponível em várias linguagens como dito anteriormente: Scala, Java, Python, R. Em um contexto geral isso é um grande ganho, pois não limita para apenas uma linguagem e o esquadrão de Cientista de Dados, Engenheiros e Analistas podem combinar PySpark e SparkR deixando escolherem suas próprias ferramentas de desenvolvimento.


Apache Spark a partir da versão 2.0.* já vem APIs de alto nível, os chamados DataFrames e Datasets. O uso de DataFrame pelo Spark se adapta muito bem aos princípios usuais de Engenharia de Software e design de aplicativos, como Testes Unitários, Modelagem de Dados e etc. O Spark também vem com uma interface SQL, que geralmente é familiar para a maioria dos programadores, engenheiros e analistas que já precisaram armazenar e consultar dados de algum lugar.


O Spark tem uma biblioteca de streaming disponível, porém não será abordado nesse artigo apesar de ser bastante atraente para aprendizagens futuras.



**Vamos começar**

Apache Spark parece ser bastante assustador (como qualquer coisa nova) e para todo estudante a pergunta várias vezes repetida é "Por onde começo?". Contudo, entender as coisas do zero é o que torna o engenheiro melhor. Isso mais a necessidade de realmente praticar o uso do Spark em algum lugar, nos leva a construir um pequeno cluster autônomo local do Spark.


*Nota: Esse tutorial será executado em Docker. Essa ferramenta dá para empacotar coisas e distribuir de uma forma escalável. A única dependência é que precisamos instalar na própria máquina o Docker Desktop. Portanto, caso queira obtê-lo, segue <a title="Docker Desktop" href="https://www.docker.com/get-started">Docker Destop</a>. Os exemplos foram executados no sistema operacional Windows 10, não tendo portanto suporte para sistema Linux.*


Nossa primeira tarefa é encontrar uma imagem Docker adequada que contenha uma versão do Java. Esse exemplo foi usado software livre tendo como uso o OpenJDK. Alpine Linux será o SO usado por ser leve e pequeno e temos opções disponíveis para rodar o OpenJDK. Vamos começar a criar nosso <i>Dockerfile</i>. 


Crie em um diretório vazio o arquivo <i>Dockerfile</i>, esse exemplo foi utilizado como auxilio a ferramenta <a title="VSC" href="https://code.visualstudio.com/download">Visual Studio Code</a>. Adicione a seguinte linha no topo do arquivo e salve:


```
FROM openjdk:8-alpine
```


Agora podemos construir nossa imagem no Docker, do qual usa o OpenJDK como base. Garanta que o WSL(simulador kernel do Linux em máquinas com sistema operacional Windows), caso contrário siga as instruções do site <a title="Microsoft WSL" href="https://docs.microsoft.com/pt-br/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package">Microsoft WSL</a> para instalação. Construa a imagem com o comando abaixo:

```
docker build .
```


Esse comando irá baixar a imagem e criar a sua própria versão. Ao final da instalação podemos notar algo parecido com a imagem abaixo:

```Successfully built 68ddc890acca```


O próximo comando tem por finalidade dá um nome para a imagem, o que torna mais fácil verificar as imagens e containers no Docker. Segue:

```
docker build -t spark:latest .
```


Agora, ao invés de usar o hash da imagem, podemos nos referir a ela usando a tag[spark:latest] que demos. Vamos testar o container usando o seguinte comando:

```
docker run -it --rm spark:latest /bin/sh
```


Essa imagem não é tudo até agora, até o momento tudo que ela contém é Java. Vamos instalar algumas utilidades que precisamos para baixar e instalar o Spark. Adicionaremos o `wget` para baixar o arquivo, o `tar` para extrair arquivos do pacote baixado,  o `bash` para rodar aplicações e o `python3` para testarmos algumas aplicações `pyspark`. Abra o arquivo `Dockerfile` e adicione a seguinte linha abaixo da linha já criada:

```
RUN apk --update add wget tar bash python3
```


Vamos reconstruir a imagem e verificar como `wget`, `tar`, `bash` e `python3` são instalados.


Agora iremos baixar e instalar o Spark. Estamos usando a última versão do Spark(3.1.1), até o momento em que escrevemos esse tutorial. Adicione a nova linha no `Dockerfile`:

```
RUN wget http://apache.mirror.anlx.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
```


Docker é inteligente e sempre reusa as camadas anteriores construídas, esse último comando fizemos o download do arquivo Spark. Uma vez construído, a imagem vai conter o arquivo Spark pronto para uso. Vamos adicionar outra linha no `Dockerfile` e reconstruir a imagem:

```
RUN tar -xzf spark-3.1.1-bin-hadoop3.2.tgz && \
cp spark-3.1.1-bin-hadoop3.2.tgz /spark && \
rm spark-3.1.1-bin-hadoop3.2.tgz
```


Ótimo, agora baixamos e instalamos o Spark como imagem Docker. Time to test it!

```
docker run --rm -it spark:latest /bin/sh
```


Para finalizar essa sessão, vamos abrir outro terminal shell e iniciar o `Spark Master`. Precisaremos de algumas opções para iniciá-lo corretamente, como por exemplo, colocar o número da porta que o `Master` escuta, o númera da porta do `WebUI` e o hostname do Master:

```
/spark-3.1.1-bin-hadoop3.2/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080
```


Ao executar o último comando vemos o log dizendo que o Master está elegido, o `Spark Master` foi iniciado com sucesso. O próximo passo é adicionar alguns `Workers` ao cluster, mas primeiro precisamos fazer algumas configurações no Master para que o Worker possa conversar com o mesmo. Para fazermos as coisas em modo simples, vamos dá ao Master um nome apropriado e expor o `Master` e o `WebUI` externamente ao Docker para que esteja disponível para visualização local. Pare o `Master` e o container usando `CTRL+C` e `CTRL+D`. Vamos executar o comando abaixo:

```
docker run --rm -it --name spark-master --hostname spark-master \
-p 7077:7077 -p 8080:8080 spark:latest /bin/sh
```


Abra um novo terminal e execute `docker ps` para ver o output gerado, algo similar com a imagem abaixo:


```
CONTAINER       ID PORTS 						NAMES

3dfc3a95f7f4 ..:7077->7077/tcp, ..:8080->8080/tcp spark-master
```


Dentro do container execute o comando abaixo para iniciar o `Spark Master` e uma vez up, poderemos ver browse http://localhost:8080

```
/spark-3.1.1-bin-hadoop3.2/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080
```


A imagem será algo parecido abaixo:

![Spark WebUI](/static/img/spark-1.png)


**Adicionando WORKER NODES**

Docker tem a sua própria rede, o qual é possível criar uma rede local para o cluster. Para criar uma rede no Docker é algo bem simples, somente seguir o comando abaixo:

```
docker network create spark_network
```


Nós não precisamos especificar nenhuma opção particular pois os padrões são adequados para esse caso. Nós vamos precisar recriar o nosso Master para configurar a nova rede. Execute:

```
docker stop spark-master 
```
ou
```
docker rm spark-master
```


Para recriar o Master na nova rede devemos adicionar a opção `-network` no `docker run`, segue:

```
docker run --rm -it --name spark-master --hostname spark-master -p 7077:7077 -p 8080:8080 --network spark_network spark:latest /bin/sh
```


Uma vez dentro do container rode o comando para expor o Master:

```
/spark-3.1.1-bin-hadoop3.2/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080
```


Não tem nenhuma diferença até aqui quando rodamos o Spark Master pela primeira vez, exceto que estamos usando uma rede definida do qual podemos tachar os Workers para fazer o cluster funcionar. Agora que o Master está up e funcionando, vamos adicionar o Worker node. Aqui é onde a mágica do Docker realmente acontece. Para criar o Worker e adicionar ao cluster, podemos simplesmente iniciar uma nova instância na mesma imagem docker e rodar o comando para iniciar o Worker. Vamos precisar dá ao Worker um novo nome, porém o comando remanescente permanece o mesmo, inicie um novo terminal e execute:

```
docker run --rm -it --name spark-worker --hostname spark-worker --network spark_network spark:latest /bin/sh
```


E então dentro do container do Spark Worker execute o comando:

```
/spark-3.1.1-bin-hadoop3.2/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8080 spark://spark-master:7077
```


Quando estiver iniciado e conectado ao Master, veremos a última linha do output:

```
INFO Worker:54 - Successfully registered with master 

spark://spark-master:7077
```

Ótimo, temos o nosso primeiro cluster Spark usando Docker!

**But Does it work?** Para verificar se funciona, podemos carregar o Master WebUI e ver o Worker node listado abaixo da seção “Workers”, isso confirma o log output do Worker tachado no Master.



![Spark WebUI](/static/img/spark-2.png)


Para termos um verdadeiro teste, vamos rodar um código Spark dentro do cluster. Vamos rodar uma nova instância do `docker image` e então podemos ter um dos exemplos nativos do Spark instalado. Outra vez, usamos o `docker image` existente e iniciamos uma nova instância para usar como driver(executores enviam requisições do aplicativo ao cluster). Isso não precisa do `--hostname`, `--name` e `-p` opção, segue:

```
docker run --rm -it --network spark_network spark:latest /bin/sh
```


Dentro do container vamos fazer o submit de uma aplicação ao cluster rodando o seguinte comando:

```
/spark-3.1.1-bin-hadoop3.2/bin/spark-submit --master spark://spark-master:7077 --class org.apache.spark.examples.SparkPi /spark-3.1.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.2.jar 1000
```


Esse exemplo executa uma aplicação para calcular o valor de `Pi` que irá usar o cluster. Enquanto a aplicação estiver rodando, podemos o status no `Spark Master WebUI`:



![Spark WebUI](/static/img/spark-3.png)

![Spark WebUI](/static/img/spark-4.png)


Uma vez completo, podemos ver o valor de Pi no log

```
Pi is roughly 3.1414459514144597
```


**Usando o Docker Compose**

Vamos verificar o `Docker Compose`, o `Docker Compose` é um utilitário bem interessante dentro do Docker que nos permite orquestrar aplicações, ou seja, não precisamos ficar rodando comandos em vários terminais. Nós essencialmente vamos colocar todos comandos e parâmetros juntos em um só lugar e rodar o simples comando `docker-compose up` e o cluster estará iniciado. Para habilitarmos, vamos criar alguns scripts para copiar a imagem e rodar no container. A primeira  coisa que iremos criar é a configuração do Spark Master. Crie um novo arquivo chamado `start-master.sh` e adicione o seguinte código:

```
#!/bin/sh

/spark-3.1.1-bin-hadoop3.2/bin/spark-class org.apache.spark.deploy.master.Master \
--ip $SPARK_LOCAL_IP \
--port $SPARK_MASTER_PORT \
--webui-port $SPARK_MASTER_WEBUI_PORT
```


Ao invés de especificar o IP, Master e WebUI portas diretamente no script, parametrizamos eles, significa que podemos providenciar os mesmos como variáveis de ambiente. Para colocar o script dentro da imagem precisaremos copiá-lo. Antes vamos garantir que o script seja uma arquivo executável, rode `chmod +x start-master.sh` no terminal para garantir. Agora adicione dentro do arquivo `Dockerfile`, abaixo da ultima linha o seguinte comando:

```
COPY start-master.sh /start-master.sh
```


Antes de reconstruir a imagem, vamos criar um script similar para o Worker também. Crie um novo arquivo chamado `start-worker.sh`, garanta que seja um arquivo executável rodando o comando `chmod +x start-master.sh` no terminal e adicione o código ao arquivo `start-worker.sh`:

```
#!/bin/sh

/spark-3.1.1-bin-hadoop3.2/bin/spark-class org.apache.spark.deploy.worker.Worker \
--webui-port $SPARK_WORKER_WEBUI_PORT \
$SPARK_MASTER
```


Adicione outro comando ao final do arquivo `Dockerfile`:

```
COPY start-worker.sh /start-worker.sh
```


Vamos reconstruir e rodar a imagem com o comando abaixo:

```
docker rmi spark
```

```
docker build -t spark:latest .
```


Vamos voltar e olhar para o `docker-compose` para orquestrar o cluster. Vamos criar um novo arquivo chamado `docker-compose.yml` e colocar o seguinte código:

```
version: "3.3"
services:
  spark-master:
    image: spark:latest
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - spark-network
    environment:
      - "SPARK_LOCAL_IP=spark-master"
      - "SPARK_MASTER_PORT=7077"
      - "SPARK_MASTER_WEBUI_PORT=8080"                
    volumes:
      - "/run/desktop/mnt/host/wsl/code:/code"
    command: 
      - "/start-master.sh"
      - "/start-delta.sh"
  spark-worker:
    image: spark:latest
    depends_on:
      - spark-master
    ports:
      - 8080
    networks:
      - spark-network
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_WEBUI_PORT=8080"
    volumes:
    - "/run/desktop/mnt/host/wsl/code:/code"
    command: 
      - "/start-worker.sh"
      - "/start-delta.sh"
networks:
  spark-network:
    driver: bridge
    ipam:
      driver: default
```


Tudo que nós especificamos até aqui é o nome da imagem, o hostname e o nome do container, expor as portas certas e tachar o nome da rede no fim do arquivo, definir algumas variáveis de ambiente e o comando a ser executado na inicialização. Também configuramos o Worker para depender do Master estar instalado e funcionando.


Para levantar o cluster, nos simplesmente rodamos o comando `docker-compose up 	.` Uma das coisas ótimas do Docker é que podemos escalar a quantidade de Workers necessária com um simples `–-scale` na opção de comando no compose. Vamos dizer que queremos 3 Worker nodes, fazemos o seguinte:

```
docker-compose up --scale spark-worker=3
```


Viu como é fácil fazer o primeiro cluster de Spark no Docker? Esse artigo demonstramos como configurar um cluster fim-a-fim, posteriormente entraremos em detalhes sobre a arquitetura Spark. Se esse post foi de alguma forma útil para você considere "bater palma".